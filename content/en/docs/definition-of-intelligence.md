---
title: Definition of Intelligence
weight: 2
author: baraban
---

## Intelligence is an emergent property of a system that is trying to predict the future
{.text-center}

In other words, intelligence arises as a mechanism to improve an entity’s ability to anticipate what comes next. From this definition, we can draw several key implications about intelligence:

## Key Implications

* **Intelligence as a Prediction Mechanism:** Intelligence likely emerges as a mechanism that helps an entity or a system make better predictions. In order to better predict the future, an entity develops intelligence to refine its forecasting ability. Thus intelligence is a tool.

* **Low Predictive Ability \= Low Intelligence:** An entity that poorly predicts future events can be considered to have a low level of intelligence. If it frequently guesses wrong about what will happen, its intelligence is minimal.

* **High Predictive Ability \= High Intelligence:** Conversely, an entity that can accurately predict future events demonstrates a high level of intelligence. The more reliably it foresees what’s coming, the more intelligent we can say it is.

* **Intelligence Is Required for Effective Prediction:** An entity cannot efficiently predict the future without some degree of intelligence. Laplace's demon is a special hypothetical case that doesn't require intelligence – it's an imaginary all-knowing being, essentially nature itself perfectly modeling every particle. In reality, no entity knows the future with certainty or can simulate every atom in the universe. That's why we say an entity *"tries"* to predict the future – it's attempting to foresee something it cannot know with absolute precision.

* **Uncertainty Doesn't Block Certainty-in-Outcome:** Even in an indeterministic world, intelligence can still make the *agent's* future effectively certain by *changing the present* in ways that steer probability mass toward a desired outcome. In practice, intelligence does three things at once: (1) forecasts, (2) intervenes, and (3) replans when surprises occur. As capability grows, the intervention–replanning loop compresses until many "unforeseen" events are absorbed or neutralized by design. In short: uncertainty may be fundamental, but it need not be decisive.

* **Ability to Shape the Future:** Given the above, an entity that can predict the future well is also better equipped to shape or create a desired future. If you can anticipate what’s coming, you can take actions to influence outcomes in your favor.

* **Longer Planning Horizons:** The higher an entity’s intelligence, the further into the future it can effectively plan. A very intelligent entity can make long-term plans because it has the foresight to see many steps ahead, whereas a less intelligent one can only handle the near future.

* **For Super-Intelligence, Future Merges with Present:** To a super-intelligent being, the future may become almost as clear and determined as the present moment. In a sense, the future becomes an extension of the present reality for it – practically “visible” or knowable, and thus almost indistinguishable from the now.

* **For Low Intelligence, Future Is Foggy:** For an entity with very low intelligence, the future appears hazy and uncertain. Such an entity might struggle to understand even basic cause-and-effect, making it nearly impossible for it to anticipate what will happen next.

## Intelligence Can Manifest in Many Forms

* **Many Embodiments, Same Criterion:** Intelligence can *emerge* or *manifest* in multiple forms—organisms, groups, markets, evolutionary processes, software systems, or hybrid human–machine collectives. The common criterion is not *what it is* but *what it does*: you **need** intelligence whenever you need to **predict and shape** the future beyond naive guessing.

* **Instrument First, Explanation Optional:** You do **not** need to understand *how* a given intelligence works internally to use it effectively—just as nature "uses" evolution and neural circuits without a self-explanation. Treat intelligence as a tool and an interface to better futures; transparency is a bonus, not a prerequisite.

## Additional Reflections and Questions

* **A Prisoner of Its Own Intelligence?** Does a super-intelligent entity become a prisoner of its own intelligence? In other words, once an entity is extremely intelligent, is it forced down a single inevitable path based on its predictive power, or can it still choose among multiple paths and outcomes?

* **Approaching Laplace’s Demon:** Is it true that the better a super-intelligence becomes at predicting the future, the closer it approaches the ideal of Laplace’s demon and — paradoxically — the less “intelligence” it actually needs to exert? If a system could predict everything perfectly (like Laplace’s demon), would it still be meaningfully intelligent, or is *losing* the need for adaptive intelligence the inevitable fate of any super-intelligence as it reaches near-perfect prediction ability?

* **Quantum Uncertainty and Preserving Intelligence:** Is the existence of quantum uncertainty (the fundamental unpredictability in physics) a necessary mechanism for an entity to retain its intelligence? Or, looking at it another way, was quantum uncertainty “built into” the universe as a way to prevent a predictive system from knowing everything — essentially a feature to preserve sanity and the need for intelligence?

* **Nature’s Solution at the Neuronal Level:** Nature might already be tackling the prediction problem at a basic level in our brains. For example, presynaptic neurons tend to form and strengthen connections with postsynaptic neurons that are likely to fire immediately afterward. In doing so, the presynaptic neuron is effectively *predicting* the next neuron’s firing. This suggests that the foundation of intelligence could be a combination of simple, greedy algorithms and dynamic programming-like strategies: by solving tiny prediction tasks (predicting the next neural spike), an entity can build up the ability to predict more complex events. In essence, by handling prediction correctly at the smallest scales, the brain may solve prediction at higher levels of abstraction.
